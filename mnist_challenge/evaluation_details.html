<p>The challenge evaluation procedure is very simple. There is a training set with labels that is provided to everyone and there is a test set. The labels of the test set will not be accessible to the participants. The leaderboard will be sorted based on how well do participant's models perform on the test set.


The training data is available in training.csv and it has a column called 'label' that shows the corresponding label. The test dataset is available in testing.csv. Use this to generate prediction labels and make a submission. Check the sample submission file submission.csv for an example submission.
</p>